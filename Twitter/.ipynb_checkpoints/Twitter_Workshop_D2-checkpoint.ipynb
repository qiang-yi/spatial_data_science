{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Data Science (GIS6307/GEO4930)\n",
    "\n",
    "\n",
    "<br>\n",
    "Instructor: Yi Qiang (qiangy@usf.edu)<br>\n",
    "Teaching Assistant: Jinwen Xu (jinwenxu@usf.edu)\n",
    "\n",
    "---\n",
    "\n",
    "# Workshop on Spatial Analysis of Twitter (Day 2)\n",
    "\n",
    "This workshop will help you to get started with the acquisition, processing, and analysis of Twitter data using data science techniques. Specifically, you will learn:\n",
    "\n",
    "- Streaming real-time tweets using Twitter Developer APIs.\n",
    "- Processing the raw tweets into an analyzable form.\n",
    "- Mapping, spatial analysis and natural language processing of Twitter data.\n",
    "\n",
    "### Prerequisites\n",
    "- Install Anaconda in your computer.\n",
    "- Activation of Twitter Developer Account and approved **Elevated Access** before the workshop.\n",
    "- Basic programming skills are recommended, but not required.\n",
    "\n",
    "\n",
    "## 1. Install Python Libraries\n",
    "\n",
    "Please open Anaconda Prompt.\n",
    "\n",
    "1. Please run the following command in Anaconda Prompt to activate the conda environment `geo` that you created on Day 1. \n",
    "    \n",
    "    - `conda activate geo`\n",
    "    \n",
    "2. Launch Jupyter Notebook using the following command:\n",
    "\n",
    "    `jupyter notebook`\n",
    "\n",
    "3. Open the downloaded .ipynb file\n",
    "\n",
    "## 2. Read and Explore Data\n",
    "\n",
    "Import libraries that are needed for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import emoji\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print full cells in dataframes\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read tweets that are streamed in the previous class. If you don't have the data, you can also download a sample dataset from [**here**](https://raw.githubusercontent.com/qiang-yi/spatial_data_science/main/Twitter/tweets_putin.csv), and save it in the default directory of Jupyter Notebook (C:\\Users\\UserName for Windows and /Users/UserName for MacOS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets_putin.csv')\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the total number of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning\n",
    "\n",
    "Raw tweets may include many meaningless phrases, symbols and characters that are hard to understand for machines. Text cleaning is the process of removing the meaningless items and prepare raw text for Natural Language Processing (NLP). Text cleaning is an important step to get meaningful analysis results from text mining. Text cleaning includes the following basic steps.\n",
    "\n",
    "- Remove punctuations, URLs, mentions and hashtags\n",
    "- Tokenization - Converting a sentence into list of words\n",
    "- Remove stopwords\n",
    "- Lammetization/stemming - Tranforming any form of a word to its root word\n",
    "\n",
    "### 3.1 Remove punctuations, URLs, mentions and hastags\n",
    "\n",
    "Punctuations URLs, mentions and hastabs may cause trouble for machines to recognize meaningful words. The first step of text cleaning is to remove these noises.\n",
    "\n",
    "The `string.punctuation` attribute contains a list of common puncutations, which will be removed. We also add some punctuations that are not included in the `string.puncuation` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining punctuations in string.punctuation and other punctuations\n",
    "punctuation = list(string.punctuation) + ['’','…','\\n']\n",
    "\n",
    "# Print the combined list of punctuations\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emojis are very popular in text messages and tweets. In Twitter, emojis are formed by punctuations, which cannot be recoginzed by machines. It will be helpful to convert emojis to words (i.e. demojize).\n",
    "\n",
    "Next, we select the first tweet in the dataset and convert the emojis to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweets['tweet'].iloc[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `emoji.demojize` to remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = emoji.demojize(text, delimiters=(' ', ' '))\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the delimiter with space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demojize\n",
    "text = text.replace(\"_\",\" \").replace('mark',\"\")\n",
    "# Split words by spaces\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine the steps of removing emojis, mentions, hashtags, URLs and punctuations in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the re library for regular expression operations.\n",
    "import re\n",
    "\n",
    "# Define a function to remove punctuation in a text string\n",
    "def remove_punct(text):\n",
    "    \n",
    "    # Convert emojis to words\n",
    "    text = emoji.demojize(text, delimiters=(' ', ' ')).replace(\"_\",\" \").replace('mark',\"\")\n",
    "\n",
    "    # Remove mentions\n",
    "    text = re.sub(\"@[A-Za-z0-9_]+\",\" \", text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(\"#[A-Za-z0-9_]+\",\" \", text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    for p in punctuation: text = text.replace(p, \" \")\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use `apply` to map the `remove_punct` function to each tweet in the `tweet` column to remove mentions, hastags, URL and punctuations. The `apply` function is a simplified syntax of a `for` loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply remove_punct to each row in the dataframe\n",
    "tweets['tweet_punct'] = tweets['tweet'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "# Preview the original tweets and processed tweets\n",
    "tweets[['tweet','tweet_punct']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tokenization\n",
    "\n",
    "Word tokenization, also known as word segmentation, divides a string of written language into its component words. White space is a good approximation of a word divider in English and many other languages with the help of some form of Latin alphabet.\n",
    "\n",
    "First, we create a function to tokenize text by non alphanumeric symbols, such as white spaces and symbols not included in puctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the tweets without punctuations, and store the tokenized tweets in a new column. We also change all characters to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "tweets['tweet_tokenized'] = tweets['tweet_punct'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "# Preview the tokenized tweets\n",
    "tweets[['tweet_punct','tweet_tokenized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Remove stop words\n",
    "\n",
    "Stop words (e.g. a, the, for...) are frequently used in English text, but carry little information. In text mining, stop words may delute the words that carry actual meanings. Removing stop words can yield to more meaningful results from text mining.\n",
    "\n",
    "The `nltk` library contains comprehensive lists of stop words in different languages. The following code get a list of stop words from `nltk.corpus.stopwords.words`, which will be removed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of stopwords from nltk, plus rt and via. \n",
    "stopword = nltk.corpus.stopwords.words('english') + ['rt', 'via','amp','get',\n",
    "                                                     'would','go','like','say',\n",
    "                                                     \"don\\'t\",'dont','need','want','think',\n",
    "                                                     'show','know','let','putin']\n",
    "\n",
    "stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `remove_stopwords` function to the tokenized tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tweet_nonstop'] = tweets['tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "\n",
    "tweets[['tweet_tokenized','tweet_nonstop']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Word stemming\n",
    "\n",
    "Word stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma (also known as Lammitization). Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\n",
    "\n",
    "First, we create a function for word stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function `stemming` to the `tweet_nonstop` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tweet_stemmed'] = tweets['tweet_nonstop'].apply(lambda x: stemming(x))\n",
    "\n",
    "tweets[['tweet_nonstop','tweet_stemmed']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the original tweets and processed tweets in the four steps. Please compare their differences to learn what has been done at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets[['tweet','tweet_punct','tweet_tokenized','tweet_nonstop','tweet_stemmed']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Frequency Analysis\n",
    "\n",
    "## 4.1 Word Cloud\n",
    "A word cloud (also known as a tag cloud) is a visual representation of words. Word cloud can highlight popular words and phrases based on frequency. Word cloud provides you with quick and simple visual insights that can lead to more in-depth analyses.\n",
    "\n",
    "Before creating word cloud, we need to break the lists in the `tweet_tokenized`, `tweet_nonstop`, and `tweet_stemmed` to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tweets_raw = tweets.tweet.sum().replace(\"\", \"\")\n",
    "\n",
    "tweets_punct = tweets.tweet_punct.sum()\n",
    "\n",
    "tweets_tokenized = tweets.tweet_tokenized.sum()\n",
    "tweets_tokenized = ' '.join([str(tweet) for tweet in tweets_tokenized])\n",
    "\n",
    "tweets_nonstop = tweets.tweet_nonstop.sum()\n",
    "tweets_nonstop = ' '.join([str(tweet) for tweet in tweets_nonstop])\n",
    "\n",
    "tweets_stemmed = tweets.tweet_stemmed.sum()\n",
    "tweets_stemmed = ' '.join([str(tweet) for tweet in tweets_stemmed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word clouds for the tweets at different processing steps. Comparing the word cloud, you can see how the 5 processing steps affect the word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a plot with subplots arranged in 3 columns * 2 rows\n",
    "fig, ax = plt.subplots(3, 2)\n",
    "\n",
    "# Create word clouds for tweets at the 5 processing steps.\n",
    "wordcloud_raw = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweets_raw)\n",
    "wordcloud_punct = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweets_punct)\n",
    "wordcloud_tokenized = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweets_tokenized)\n",
    "wordcloud_nonstop = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweets_nonstop)\n",
    "wordcloud_stemmed = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweets_stemmed)\n",
    "\n",
    "# Display the word cloud to the subplots one by one\n",
    "ax[0,0].imshow(wordcloud_raw, interpolation='bilinear')\n",
    "ax[0,0].set_title('Raw tweets', fontsize=16)\n",
    "ax[0,0].axis('off')\n",
    "\n",
    "ax[0,1].imshow(wordcloud_punct, interpolation='bilinear')\n",
    "ax[0,1].set_title('Tweets after remove punctuation',fontsize=16)\n",
    "ax[0,1].axis('off')\n",
    "\n",
    "ax[1,0].imshow(wordcloud_tokenized, interpolation='bilinear')\n",
    "ax[1,0].set_title('Tokenized tweets',fontsize=16)\n",
    "ax[1,0].axis('off')\n",
    "\n",
    "ax[1,1].imshow(wordcloud_nonstop, interpolation='bilinear')\n",
    "ax[1,1].set_title('Tweets without stop words',fontsize=16)\n",
    "ax[1,1].axis('off')\n",
    "\n",
    "ax[2,0].imshow(wordcloud_stemmed, interpolation='bilinear')\n",
    "ax[2,0].set_title('Tweets after stemming',fontsize=16)\n",
    "ax[2,0].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Word Frequency in Bar Chart\n",
    "\n",
    "In addition to word cloud, bar chart is useful graph to show word frequency. To create a bar chart, we need to combine all cleaned tweets and count the number of identical words. We can do this using `collections.Counter`. Finally, we will get the 15 most frequently appeared words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Combine all tweets in the tweet_stemmed column\n",
    "all_tweets = tweets.tweet_stemmed.sum()\n",
    "\n",
    "# Count the numbers of identical words in the tweets.\n",
    "counts = collections.Counter(all_tweets)\n",
    "\n",
    "# Get the 15 most frequently appeared words\n",
    "most_common_words = counts.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `most_common_words` to a dataframe, which is easier for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "frequent_tweets = pd.DataFrame(most_common_words, columns=['words', 'count'])\n",
    "\n",
    "# Print the dataframe\n",
    "frequent_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bar chart to display the most frequently appeared words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot horizontal bar graph\n",
    "frequent_tweets[1:16].sort_values(by='count').plot.barh(x='words',\n",
    "                      y='count',\n",
    "                      ax=ax)\n",
    "\n",
    "ax.set_title(\"Most Frequent Words in Tweets\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sentiment Analysis\n",
    "\n",
    "As a common text mining technique, Sentiment analysis can be defined as a process that automates mining of attitudes, opinions, views and emotions from text, speech, tweets and database sources through Natural Language Processing (NLP). Sentiment analysis involves classifying opinions in text into categories like \"negative\" (score: -1), \"neutral\" (score 0)\" or \"positive\" (score: 1). Sentiment analysis is also referred to as subjectivity analysis, opinion mining, and appraisal extraction.\n",
    "\n",
    "Before sentiment analysis, we first break the lists in `tweet_stemmed` to bare strings of words separated by spaces, and store the converted tweets in a new column `tweet_stemmed2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of words to strings\n",
    "tweets['tweet_stemmed2'] = tweets['tweet_stemmed'].apply(lambda x: ' '.join([str(tweet) for tweet in x]))\n",
    "\n",
    "# Preview the converted strings\n",
    "tweets[['tweet_stemmed','tweet_stemmed2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `textblob` pacakge to calculate sentiment scores of the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Calculate sentiment scores of the tweets\n",
    "sentiment_objects = [TextBlob(tweet) for tweet in tweets.tweet_stemmed2]\n",
    "\n",
    "# Print the sentiment scores of the first 20 tweets\n",
    "[object.polarity for object in sentiment_objects][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the sentiment scores in a new column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentiment'] = [object.polarity for object in sentiment_objects]\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take a look at some tweets with most positive sentiment (-1). \n",
    "\n",
    "To do so, we sort the dataframe based on sentiment in an descending order, and then preview the first 5 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.sort_values(by='sentiment', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a histogram to show distribution of  sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a canvas in a specific size\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot histogram \n",
    "plt.hist(tweets['sentiment'], bins=20,edgecolor='k', alpha=0.65)\n",
    "plt.axvline(tweets['sentiment'].mean(), color='red', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentiment analysis, many words don't have a positive/negative sentiment, and are assigned neutral (0) sentiment. So a large number of tweets have a 0 sentiment, creating a high bar in the middle. \n",
    "\n",
    "Let's check how many tweets have 0 sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} out of {} tweets have neutral (zero) sentiment.\".format(len(tweets[tweets['sentiment']==0]), len(tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most tweets have a neutral sentiment. So it makes sense to remove neutral tweets and only keeps tweets with a positive and negative sentiment. The following code select tweets with non-zero sentiment and store them in `tweets2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2 = tweets[tweets['sentiment']!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the histogram with non-zero sentiment tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a canvas in a specific size\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot histogram \n",
    "plt.hist(tweets2['sentiment'], bins=20,edgecolor='k', alpha=0.65)\n",
    "plt.axvline(tweets2['sentiment'].mean(), color='red', linewidth=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create heat map for sentiment\n",
    "\n",
    "Heat map (also known as kernel density map) is a common approach to visualise clusters of points. Heat map use color gradients to display density variation of points. Other than treating the points equally, heat map can also include a \"population\" attribute to weigh the points.\n",
    "\n",
    "In this task, we will create a heat map to display clusters of tweets using sentiment scores as the population field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Calculating centroids of geotags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat map applies to point data. So we use centroids of geotags (bounding box) to represent the tweet locations.\n",
    "\n",
    "The `geotag` column contains bounding boxes in the Well-Known Text (WKT) format as strings. Next, we will use the `json` package to convert the WKT strings to lists, and store the lists in a new column `geotag2`. The lists are easier to access coordinates of the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the string to lists\n",
    "tweets2['geotag2'] = tweets2['geotag'].apply(lambda st: json.loads(st))\n",
    "\n",
    "tweets2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can access coordiantes in `geotag2` to calculate coordinates of centroids of the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coordinates of bounding box centroid\n",
    "tweets2['point']  = tweets2['geotag2'].apply(lambda s: [(s[0][1]+s[2][1])/2,(s[0][0]+s[2][0])/2])\n",
    "\n",
    "# Get the latitude of the centroid \n",
    "tweets2['lat']  = tweets2['geotag2'].apply(lambda s: (s[0][1]+s[2][1])/2)\n",
    "\n",
    "# Get the longitude of the centroid \n",
    "tweets2['lon']  = tweets2['geotag2'].apply(lambda s: (s[0][0]+s[2][0])/2)\n",
    "\n",
    "# Preview the geotweets\n",
    "tweets2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Create Heat Maps for positive and negative sentiment tweets\n",
    "\n",
    "Split the dataframe to two dataframes with positive and negative sentiment. Ideally, we should create a kernel density map using sentiment as the population parameter (as the figure below). However, `folium` does not support negative population. So we will work around it by mapping positive and negative sentiment in separate layers and overlay them in a map.\n",
    "\n",
    "![](https://raw.githubusercontent.com/qiang-yi/spatial_data_science/main/image/twitter/kernel.jpg)\n",
    "\n",
    "![](https://raw.githubusercontent.com/qiang-yi/spatial_data_science/main/image/twitter/kde.png)\n",
    "\n",
    "We first separate positive and negative tweets, and then convert negative scores to positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tweets with positive (>1) and negative (<1) sentiment \n",
    "positive = tweets2.loc[tweets2['sentiment']>0,['lat','lon','sentiment']]\n",
    "negative = tweets2.loc[tweets2['sentiment']<0,['lat','lon','sentiment']]\n",
    "\n",
    "# Convert the positve tweets to an numpy array\n",
    "positive = np.array(positive)\n",
    "\n",
    "# Convert the negative tweets to an numpy array\n",
    "negative['sentiment'] = negative['sentiment'].abs()\n",
    "negative = np.array(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create heat maps for positive and negative sentiment tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import branca.colormap as cm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "lon, lat = tweets2['lon'].mean(), tweets2['lon'].mean()\n",
    "zoom_level = 1\n",
    "\n",
    "steps = 20\n",
    "\n",
    "\n",
    "m = folium.Map([lon, lat], tiles='stamentoner', zoom_start=zoom_level)\n",
    "\n",
    "\n",
    "#colormap_pos=cm.linear.Blues_09.scale(0,1).to_step(steps)\n",
    "colormap_pos = cm.LinearColormap(colors=['white','blue'], index=[0,1],vmin=0,vmax=1)\n",
    "\n",
    "gradient_map_pos=defaultdict(dict)\n",
    "for i in range(steps):\n",
    "    gradient_map_pos[1/steps*i] = colormap_pos.rgb_hex_str(1/steps*i)\n",
    "#colormap_pos.add_to(m) #add color bar at the top of the map\n",
    "\n",
    "colormap = cm.LinearColormap(colors=['red','white','blue'], index=[-1,0,1],vmin=-1,vmax=1, caption='Total Standard deviation at the point[mm]')\n",
    "\n",
    "data_pos = (positive).tolist()\n",
    "HeatMap(data_pos,gradient = gradient_map_pos,min_opacity=0.5).add_to(folium.FeatureGroup(name='Positive').add_to(m))\n",
    "colormap.add_to(m)\n",
    "\n",
    "\n",
    "#colormap_neg=cm.linear.Reds_09.scale(0,1).to_step(steps)\n",
    "colormap_neg = cm.LinearColormap(colors=['white','red'], index=[0,1],vmin=0,vmax=1)\n",
    "\n",
    "gradient_map_neg=defaultdict(dict)\n",
    "for i in range(steps):\n",
    "    gradient_map_neg[1/steps*i] = colormap_neg.rgb_hex_str(1/steps*i)\n",
    "#colormap_neg.add_to(m) #add color bar at the top of the map\n",
    "\n",
    "data_neg = (negative).tolist()\n",
    "HeatMap(data_neg,gradient = gradient_map_neg,min_opacity=0.5).add_to(folium.FeatureGroup(name='Negative').add_to(m))\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
