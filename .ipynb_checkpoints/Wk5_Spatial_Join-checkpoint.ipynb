{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spatial Data Science (GEO6119)\n",
    "\n",
    "---\n",
    "\n",
    "# Week 5: Attribute Join and Spatial Join\n",
    "\n",
    "\n",
    "<br>\n",
    "Instructor: Yi Qiang (qiangy@usf.edu)<br>\n",
    "\n",
    "This lecture is modified from this [tutorial](https://pandas.pydata.org/docs/user_guide/merging.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concatenating DataFrames\n",
    "First, we import pandas and geopandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Simple Concatenation\n",
    "\n",
    "Create three data frames: df1, df2 and df3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df1\n",
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    },\n",
    "    index=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "# Create df2\n",
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A4\", \"A5\", \"A6\", \"A7\"],\n",
    "        \"B\": [\"B4\", \"B5\", \"B6\", \"B7\"],\n",
    "        \"C\": [\"C4\", \"C5\", \"C6\", \"C7\"],\n",
    "        \"D\": [\"D4\", \"D5\", \"D6\", \"D7\"],\n",
    "    },\n",
    "    index=[4, 5, 6, 7],\n",
    ")\n",
    "\n",
    "# Create df3\n",
    "df3 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A8\", \"A9\", \"A10\", \"A11\"],\n",
    "        \"B\": [\"B8\", \"B9\", \"B10\", \"B11\"],\n",
    "        \"C\": [\"C8\", \"C9\", \"C10\", \"C11\"],\n",
    "        \"D\": [\"D8\", \"D9\", \"D10\", \"D11\"],\n",
    "    },\n",
    "    index=[8, 9, 10, 11],\n",
    ")\n",
    "\n",
    "# Print the three dataframes\n",
    "display(df1)\n",
    "display(df2)\n",
    "display(df3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the three data frame into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes into a list\n",
    "ls_df = [df1, df2, df3]\n",
    "\n",
    "# Contatenate ls_df into a dataframe\n",
    "result = pd.concat(ls_df)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concatenation process can be illustrated as follows:\n",
    "\n",
    "![](image/wk5/wk5_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Adding an additional key in the concatenation\n",
    "\n",
    "Suppose we wanted to associate specific keys with each of the pieces of the chopped up DataFrame. We can use the `keys` parameter in the function. In this way, we can retrieve the original dataframes before the concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(ls_df, keys=[\"df1\", \"df2\", \"df3\"])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get df1 from the concatenated dataframe 'result'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[\"df1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Concatenate in different axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The axis parameter determines which dimension you want to concatenate dataframe. The default value of axis is 0, meaning the concatenation is conducted vertically (in rows).\n",
    "\n",
    "If we change axis to 1, we concatenate the dataframes horizontally (in columns). The following example shows the concatenated dataframe by setting axis = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the three dataframes again.\n",
    "display(df1)\n",
    "display(df2)\n",
    "display(df3)\n",
    "\n",
    "# Concatenate the three dataframes\n",
    "result = pd.concat(ls_df,axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we reset the indices of df2 and df3 to start from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df2.reset_index(drop=True),df3.reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Set logic on axes\n",
    "\n",
    "When gluing together multiple DataFrames, you have a choice of how to handle the other axes (other than the one being concatenated). This can be done in the following two ways:\n",
    "\n",
    "- Take the union of them all, join='outer'. This is the **default** option as it results in zero information loss.\n",
    "\n",
    "- Take the intersection, join='inner'.\n",
    "\n",
    "Now, we create a new dataframe df4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame(\n",
    "    {\n",
    "        \"B\": [\"B2\", \"B3\", \"B6\", \"B7\"],\n",
    "        \"D\": [\"D2\", \"D3\", \"D6\", \"D7\"],\n",
    "        \"F\": [\"F2\", \"F3\", \"F6\", \"F7\"],\n",
    "    },\n",
    "    index=[2, 3, 6, 7],\n",
    ")\n",
    "\n",
    "display(df1)\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df4], axis = 1)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure illustrates the concatenation.\n",
    "\n",
    "![](image/wk5/wk5_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try to re-do the concatenation, with inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df4], axis=1, join=\"inner\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure illustrates the concatenation.\n",
    "\n",
    "![](image/wk5/wk5_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also concatenate df1 and df4 vertically (axis = 0). The default join is outer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df4])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try inner join in the 0 axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df4],join = 'inner')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attribute Join\n",
    "\n",
    "Using the `merge` function, you can join dataframes with matching values in one or multiple columns (keys).\n",
    "\n",
    "The following code create df1 and df2, and merge them using the index: rows with identical index are horizontally combined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame(\n",
    "    {\n",
    "        \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "right = pd.DataFrame(\n",
    "    {\n",
    "        \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(left)\n",
    "display(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, on=\"key\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Inner Join\n",
    "\n",
    "Next, we join two data frames that have different but overlapping values in keys.\n",
    "\n",
    "In this example, the column names of keys in the left are different (key1 and key2). You can use `left_on` and `right_on` parameters to determine different keys on the left and right.\n",
    "\n",
    "The default join type is 'inner', meaning that only the matching keys in left and right dataframes are combined (the intersection). The rows with unmatched keys are not discarded. The following graph illustrates different joins.\n",
    "\n",
    "![](image/wk5/joins.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame(\n",
    "    {\n",
    "        \"key1\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "right = pd.DataFrame(\n",
    "    {\n",
    "        \"key2\": [\"K2\", \"K3\", \"K4\", \"K5\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(left)\n",
    "display(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the two data frames by matching key1 in the left dataframe and key2 key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, left_on = \"key1\", right_on = \"key2\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Outer Join\n",
    "\n",
    "Outer join not only combine the rows with matching keys, but also the rows that don't have matching keys, and adding NaN to the columns from the other dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(left)\n",
    "display(right)\n",
    "\n",
    "result = pd.merge(left, right, how = \"outer\", left_on = 'key1', right_on = 'key2')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Left and Right Join\n",
    "\n",
    "Left join keeps all rows in the left data frame, and join rows in the right dataframe that match the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(left)\n",
    "display(right)\n",
    "\n",
    "result = pd.merge(left, right, how = \"left\", left_on = 'key1', right_on = 'key2')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right join does the reverse, keeping all rows in the right dataframe and add matching rows in the left dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how = \"right\", left_on = 'key1', right_on = 'key2')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Join with multiple keys\n",
    "\n",
    "You can also join dataframes using two keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame(\n",
    "    {\n",
    "        \"state\": [\"FL\", \"FL\", \"NY\", \"NH\"],\n",
    "        \"county\": [\"Hillsborough County\", \"Washington County\", \"Jefferson County\", \"Hillsborough County\"],\n",
    "        \"Population\": [1451000, 25318, 116721, 422937],\n",
    "        \"Land area\": [1020, 583, 1269, 876],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "right = pd.DataFrame(\n",
    "    {\n",
    "        \"state\": [\"GA\", \"NY\", \"OH\", \"FL\"],\n",
    "        \"county\": [\"Jefferson County\", \"Jefferson County\", \"Washington County\", \"Hillsborough County\"],\n",
    "        \"Founded\": [1796,1805,1788,1834],\n",
    "        \"Water area\": [3.2,589,8,246],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(left)\n",
    "display(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the left and right data frames using 'state' and 'county' as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, on = [\"state\", \"county\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute join (above) combines two dataframes by matching keys. Spatial join combines two geodataframes by spatial relations (e.g. intersect, within, contain...).\n",
    "\n",
    "First, download airbnb listings and voting district boundaries in Stockholm. The data can be found in [Inside Airbnb](http://insideairbnb.com/get-the-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download Stockholm airbnb listings\n",
    "urllib.request.urlretrieve(\"http://data.insideairbnb.com/sweden/stockholms-l%C3%A4n/stockholm/2022-06-25/visualisations/listings.csv\",\n",
    "                  \"other/airbnb_listing.csv\")\n",
    "\n",
    "# Download the neighborhood boundary in Stockholm.\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/shakasom/spatialjoin-python/master/data/stockholm_areas.geojson\",\n",
    "                  \"other/district.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv and geojson file into a dataframe and a geodataframe respectively.\n",
    "\n",
    "> Note: you need to specify the coordinate system (CRS) when creating/loading the geodataframes. Otherwise, the created maps won't be projected correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the airbnb listings\n",
    "listings = pd.read_csv(\"other/airbnb_listing.csv\")\n",
    "\n",
    "# Convert the listings (csv) to Geopandas Geodataframe\n",
    "gdf_listings = gpd.GeoDataFrame(listings, geometry = gpd.points_from_xy(listings.longitude, listings.latitude), crs='EPSG:4326')\n",
    "\n",
    "# Read the neighbourhood boundaries\n",
    "districts = gpd.read_file('other/district.geojson',crs = 'EPSG:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the district boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the airbnb listing locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_listings.plot(color = 'black', markersize = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the above two layers into one map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change the default figure size to 10 inches by 10 inches\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "\n",
    "ax = districts.plot(facecolor = 'none', edgecolor='red')\n",
    "\n",
    "gdf_listings.plot(ax = ax, color = 'blue', markersize = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add basemap beneath the two layers, to give background information of the study area. \n",
    "\n",
    "We can use the convextily library to add base maps. Since contextily can only display map tiles in a project coordinate system (epsg:4326 is a geographic coordinate system (lat&lon)). So we need to reproject the airbnb listings and neighborhoods into a projected coordinate system for the mapping, i.e., SWEREF 99 TM Zone system (epsg:3006)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import contextily\n",
    "import contextily as cx\n",
    "\n",
    "# Add the neighborhood boundaries\n",
    "ax = districts.to_crs(epsg=3006).plot(facecolor = 'none', edgecolor='red')\n",
    "\n",
    "# Add airbnb listings\n",
    "gdf_listings.to_crs(epsg=3006).plot(ax = ax, color = 'blue', markersize = 1)\n",
    "\n",
    "# Add the basemap\n",
    "cx.add_basemap(ax = ax, source = cx.providers.Stamen.TonerLite, crs = 'EPSG:3006')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the airbnb data, we use spatial join (`geopandas.sjoin`) to join the airbnb listings (points) to the neighborhood boundaries (polygons).\n",
    "\n",
    "The `op` parameter defines the spatial relation for the joining. `op = 'within'` means that the airbnb listings need to be within a district so it can be joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial join airbnbs to district\n",
    "districts_j = gpd.sjoin(districts, gdf_listings, op='contains')\n",
    "\n",
    "# Preview the joined data\n",
    "districts_j.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we print the shapes of gdf_listings, districts and districts_joined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of gdf_listings: '+ str(gdf_listings.shape))\n",
    "\n",
    "print('shape of districts: '+ str(districts.shape))\n",
    "\n",
    "print('shape of districts_j: '+ str(districts_j.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the rows where fid is 5287, in districts and districts_joined. You can see that districts_joined has multiple rows of fid 5287, while districts has only 1. This is because each airbnb listing contained in fid 5287 becomes a row after spatial join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(districts[districts['fid'] == 5287])\n",
    "\n",
    "display(districts_j[districts_j['fid'] == 5287])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "districts_join has many rows that share identical geometries (polygons) for different airbnb lisitings. We can collapse the rows with identical polygons (same fid) into one, and keep aggregated values (e.g. count, mean) of airbnb listings in the polygon.\n",
    "\n",
    "We can do this by groupby the fid column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = districts_j.groupby(by='fid').agg(['count'])['Deso']\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we join (attribute join) the airbnb count in each fid (polygon) back to the district boundary, using fid as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview districts\n",
    "display(districts.head())\n",
    "\n",
    "# Preview count\n",
    "display(count.head())\n",
    "\n",
    "# Preview district_count\n",
    "districts_count = districts.merge(count,on='fid')\n",
    "display(districts_count.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate airbnb density in each district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_count['density'] = districts_count['count']/districts_count['geometry'].area\n",
    "\n",
    "districts_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a choropleth map to show airbnb density in district.\n",
    "\n",
    "- You may choose different color schemes (cmap) from [here](https://matplotlib.org/stable/tutorials/colors/colormaps.html)\n",
    "- You may refer this [documentation](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.plot.html) choose different classification method (quantiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the neighborhood boundaries\n",
    "ax = districts_count.to_crs(epsg=3006).plot(column='density', cmap='OrRd', scheme='quantiles',alpha=0.6);\n",
    "\n",
    "# Add the basemap\n",
    "cx.add_basemap(ax = ax, source = cx.providers.Stamen.TonerLite, crs = 'EPSG:3006')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"C:/Users/yi/Documents/USF_work/Teaching/GEO6119/labs/lab5_data/FL_tracts.shp\")\n",
    "\n",
    "pop = pd.read_csv(\"C:/Users/yi/Documents/USF_work/Teaching/GEO6119/labs/lab5_data/population.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['GEOID'] = gdf['GEOID'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf.merge(pop, left_on='GEOID',right_on='Geography')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
